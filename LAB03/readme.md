# Laboratorio 03: MonitorizaciÃ³n y gestiÃ³n de alertas en Kubernetes

## ðŸŽ¯ Objetivo general

Desplegar herramientas de monitorizaciÃ³n y alertado (Prometheus, AlertManager), generar alertas bÃ¡sicas y analizar eventos mediante logs. El objetivo es comprender cÃ³mo observar el estado del clÃºster y reaccionar ante fallos.

## ðŸ§° Requisitos previos

* ClÃºster local en ejecuciÃ³n (kind o real)
* `kubectl`, `helm`, `stern` instalados
* Acceso a Internet desde los nodos para descarga de charts

---

## ðŸ”¬ Fases del laboratorio

### Fase 1: Despliegue del stack Prometheus + AlertManager

**ðŸŽ¯ Objetivo:** Tener Prometheus y AlertManager operativos en el clÃºster.

**ðŸ”§ Pasos:**

1. AÃ±adir el repo de Helm:

   ```bash
   helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
   helm repo update
   ```
2. Crear namespace:

   ```bash
   kubectl create namespace monitoring
   ```
3. Instalar Prometheus stack:

   ```bash
   helm install prom prometheus-community/kube-prometheus-stack -n monitoring
   ```
4. Esperar a que todos los pods estÃ©n en estado Running.

**ðŸ”¥ Retos:**

* Verifica los endpoints de Prometheus y AlertManager con `kubectl port-forward`
* Accede a las interfaces web desde localhost

---

### Fase 2: CreaciÃ³n de alertas por estado de pods y uso de CPU

**ðŸŽ¯ Objetivo:** Aprender a crear reglas de alerta bÃ¡sicas en Prometheus.

**ðŸ”§ Pasos:**

1. Localiza el `prometheus-prometheus-rulefiles` ConfigMap generado.
2. Crea un nuevo archivo con reglas personalizadas:

   ```yaml
   groups:
   - name: custom.rules
     rules:
     - alert: HighPodRestart
       expr: rate(kube_pod_container_status_restarts_total[5m]) > 0.1
       for: 1m
       labels:
         severity: warning
       annotations:
         summary: "Alerta: Reinicios altos en pods"
         description: "{{ $labels.pod }} ha reiniciado mÃ¡s de una vez por minuto."
   ```
3. Aplica el archivo mediante un nuevo ConfigMap y reinicia Prometheus si es necesario.

**ðŸ”¥ Retos:**

* Forzar una alerta haciendo que un pod falle
* Comprobar si aparece en AlertManager

---

### Fase 3: SimulaciÃ³n de condiciones anÃ³malas

**ðŸŽ¯ Objetivo:** Generar condiciones que disparen las alertas.

**ðŸ”§ Pasos:**

1. Desplegar un pod que falle de forma intencional:

   ```bash
   kubectl run failer --image=busybox --restart=Never --command -- sh -c "exit 1"
   ```
2. Repetirlo o hacer loops para generar eventos frecuentes.

**ðŸ”¥ Retos:**

* Dejar corriendo una carga que agote CPU o memoria
* Ver cÃ³mo reacciona Prometheus

---

### Fase 4: RevisiÃ³n de logs y eventos con stern y kubectl

**ðŸŽ¯ Objetivo:** Ver los logs de pods con problemas y correlacionarlos con alertas.

**ðŸ”§ Pasos:**

1. Usar `stern` para ver logs en tiempo real:

   ```bash
   stern failer
   ```
2. Revisar eventos del pod:

   ```bash
   kubectl describe pod failer
   ```
3. Eliminar el pod para limpiar el entorno:

   ```bash
   kubectl delete pod failer
   ```

**ðŸ”¥ Retos:**

* Comprobar timestamps de logs frente a tiempos de alerta
* Ver cÃ³mo se actualiza la interfaz de Prometheus

---

## âœ… Validaciones finales

* Prometheus y AlertManager estÃ¡n desplegados y accesibles
* Se han generado alertas personalizadas
* Se han producido fallos y correlacionado eventos/logs
* El alumno entiende cÃ³mo interpretar las mÃ©tricas
