### Fase 3: Simular fallo de nodo en entorno VM (Hyper-V)

---

### ðŸŽ¯ Objetivo

Simular una situaciÃ³n de fallo en un nodo del clÃºster para observar cÃ³mo Kubernetes responde y maneja la alta disponibilidad mediante redistribuciÃ³n y reprogramaciÃ³n de pods.

---

### ðŸ§° Requisitos

* ClÃºster distribuido en VMs o entorno Hyper-V
* MÃ­nimo 2 nodos configurados
* Acceso al hipervisor (Hyper-V, VirtualBox, Proxmox, etc.)

---

### ðŸ”§ Pasos

1. Verificar distribuciÃ³n de pods por nodo:

   ```bash
   kubectl get pods -o wide
   ```

2. Identificar el nodo que se apagarÃ¡:

   ```bash
   kubectl get nodes -o wide
   ```

3. Simular fallo: detener la VM del nodo seleccionado desde el hipervisor (no usar `kubectl drain`).

4. Esperar entre 30s y 2m y comprobar eventos:

   ```bash
   kubectl get nodes
   kubectl describe node <nombre-del-nodo>
   kubectl get pods -o wide
   ```

5. Verificar si los pods afectados han sido reprogramados en los nodos restantes.

6. Reiniciar la VM desde el hipervisor y comprobar si el nodo vuelve a estado `Ready`.

---

### ðŸ”¥ Retos

* **Simula el fallo de varios nodos simultÃ¡neamente y mide el impacto**
  ðŸ’¡ Observa si el clÃºster logra reprogramar los pods o si se bloquea por falta de capacidad.

* **Relaciona los eventos del clÃºster con las alertas disparadas por el fallo**
  ðŸ’¡ Usa `kubectl get events` y revisa Prometheus/AlertManager.

* **Crea un Deployment tolerante a fallos mediante PodDisruptionBudgets y replicas distribuidas**
  ðŸ’¡ Asegura que no todo quede en un Ãºnico nodo.

---

### âœ… Validaciones

* Al detener un nodo, su estado pasa a `NotReady` o desaparece temporalmente
* Los pods del nodo fallido se reprograman en otros nodos (si hay capacidad)
* El nodo vuelve a estado `Ready` tras el reinicio
* Se observa el comportamiento de alta disponibilidad bÃ¡sico en Kubernetes
